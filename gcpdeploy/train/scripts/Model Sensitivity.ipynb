{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPcnhbbPjezL"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Bike Weather Data Mar 2024 (1).csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "TWrjfCsj8hsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess the data:\n",
        "    - Resample the data hourly by start_station_name.\n",
        "    - Encode categorical variables.\n",
        "    - Scale numerical features.\n",
        "\n",
        "    Args:\n",
        "    df (pandas.DataFrame): Raw input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Preprocessed features (X), target (y), and the preprocessor pipeline.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df['started_at'] = pd.to_datetime(df['started_at'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "        # Drop rows where 'started_at' is NaT\n",
        "        df = df.dropna(subset=['started_at'])\n",
        "\n",
        "        # Set the datetime column as the index for resampling\n",
        "        df.set_index('started_at', inplace=True)\n",
        "\n",
        "        # Resample data by hour for each station\n",
        "        hourly_station_data = df.groupby('start_station_name').resample('h').agg({\n",
        "            'month': 'first',                # Keep first value (since it's constant for each hour)\n",
        "            'hour': 'first',                 # Same as above\n",
        "            'day_name': 'first',             # Same\n",
        "            'duration': 'sum',               # Sum durations for the hour\n",
        "            'distance_km': 'sum',            # Sum distances for the hour\n",
        "            'Temperature (°F)': 'mean',      # Average temperature\n",
        "            'Humidity': 'mean',              # Average humidity\n",
        "            'Wind Speed': 'mean',            # Average wind speed\n",
        "            'Precip.': 'sum',                # Total precipitation for the hour\n",
        "            'Condition': 'first',            # Keep first condition as representative\n",
        "            'bike_undocked': 'sum'            # Sum undocked bikes\n",
        "        }).reset_index()\n",
        "\n",
        "        # Prepare features (X) and target (y)\n",
        "        X = hourly_station_data.drop(columns=['bike_undocked'])\n",
        "        y = hourly_station_data['bike_undocked']\n",
        "\n",
        "        # One-hot encode categorical variables\n",
        "        X = pd.get_dummies(X, columns=['day_name', 'Condition'], drop_first=True)\n",
        "\n",
        "        # Frequency encode the 'start_station_name' column\n",
        "        station_freq = X['start_station_name'].value_counts().to_dict()\n",
        "        X['start_station_name'] = X['start_station_name'].map(station_freq).fillna(0)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        X = X.drop(columns=['started_at'])\n",
        "\n",
        "        # Fill any remaining missing values with 0\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        # Split the data chronologically into train, validation, and test sets\n",
        "        train_size = int(0.7 * len(X))  # 70% for training\n",
        "        val_size = int(0.15 * len(X))   # 15% for validation\n",
        "        test_size = len(X) - train_size - val_size  # 15% for test\n",
        "\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "        X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "        X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    except Exception as e:\n",
        "        raise"
      ],
      "metadata": {
        "id": "zsBB1km8jydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Bike Weather Data Mar 2024 (1).csv'\n",
        "bike_data = pd.read_csv(file_path)\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = preprocess_data(bike_data)"
      ],
      "metadata": {
        "id": "2Q_GBque2q3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values and categorical features with a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numeric values with mean\n",
        "            ('scaler', StandardScaler())  # Scale numeric features\n",
        "        ]), X_train.select_dtypes(include=['float64', 'int64']).columns),\n",
        "\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Impute missing categorical values\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
        "        ]), X_train.select_dtypes(include=['object']).columns)\n",
        "    ])"
      ],
      "metadata": {
        "id": "Qo0q7h0N7_a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessor to the features\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train_processed, y_train)\n",
        "\n",
        "# Predict on the test set and calculate mean squared error\n",
        "y_pred = model.predict(X_test_processed)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Initialize SHAP explainer with the trained Decision Tree model\n",
        "explainer = shap.Explainer(model, X_train_processed)\n",
        "\n",
        "# Compute SHAP values for the test set\n",
        "shap_values = explainer(X_test_processed)\n",
        "\n",
        "# Extract and print SHAP values for each feature\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "# Calculate the mean absolute SHAP value for each feature to get feature importance\n",
        "shap_importance = shap_df.abs().mean(axis=0).sort_values(ascending=False)\n",
        "\n",
        "# Display the features and their associated importance values\n",
        "print(\"Feature Importance based on SHAP:\")\n",
        "print(shap_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjGpPFvU8VRS",
        "outputId": "c8c7cf20-677d-44e4-8d0c-d51ceaf4ed9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.3611535311953611\n",
            "Feature Importance based on SHAP:\n",
            "num__month                 0.409370\n",
            "num__duration              0.242753\n",
            "num__distance_km           0.114153\n",
            "num__start_station_name    0.043923\n",
            "num__hour                  0.037980\n",
            "num__Temperature (°F)      0.022040\n",
            "num__Humidity              0.020734\n",
            "num__Wind Speed            0.017148\n",
            "num__Precip.               0.000737\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVlmVHp38btx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}